
# A simple example that copies all tables from a SQLite database. The first few settings are
# required for all connectors: a name, the connector class to run, and the maximum number of
# tasks to create:
name=test-source-sqlite-jdbc-autoincrement
connector.class=io.confluent.connect.jdbc.JdbcSourceConnector
tasks.max=1

# The remaining configs are specific to the JDBC source connector. In this example, we connect to a
# SQLite database stored in the file test.db, use and auto-incrementing column called 'id' to
# detect new rows as they are added, and output to topics prefixed with 'test-jdbc-'

##################################################
# 메모
##################################################
# - ms sql server 연결
# 	=> https://docs.confluent.io/kafka-connect-jdbc/current/source-connector/source_config_options.html
# 	=> jdbc 커넥터 다운로드시 포함된 예제 속성파일은 완전하지 않다. 위 링크 참고하여 필요한 세팅값을 더 넣어주자.

# 	=> 이슈1: wsl에서 커넥트 테스트 실행시킬때 windows에 있는 db 주소는 localhost가 아님. 네트워크 인터페이스 정확하게 넣어줘야 한다.
# 	=> 이슈2: 사용자가 로그인하지 못했습니다.
# 		=> 커넥터 속성파일에 connection.user, connection.password 등록 필요. 기본 속성파일 템플릿에 없음. 위 링크 참고
# 	=> 이슈3: column.name: id 등으로 데이터 읽을 기준을 잡아준다.
# 	=> 이슈4: mode: 모드에 따라 어떤식으로 db값을 가져올지 정한다. id 순차대로 새로운것, 시간순대로 새로운것, 기존것 모두(bulk) 등
# 		=> bulk 모드의 경우, db에 polling 할 때마다 전체 데이터를 긁어온다.
# 	=> 이슈5: 기본 에제의 connection.url에서 'instance=SQLSERVER;' 부분을 지우기 (실행되다 멈춤)
#   => ms sql server 설정시 이슈 일부: https://xzio.tistory.com/73  (글 맨아래쪽 참고)
#


# ㅇ- 카프카 브로커에 데이터가 들어갔는지 확인
# 	- 카프카 명령 모음 (https://pinggoopark.tistory.com/6)
# 	ㅇ- 토픽 리스트 확인 : $./bin/kafka-topics.sh --list --bootstrap-server localhost:9092
# 		=> 미리 토픽을 등록해두지 않으면 connect 설정값에 따라 자동 등록되므로, 새롭게 토픽이 추가된 것을 확인한다.
# 	ㅇ- 토픽의 데이터 확인 (컨슈머): $./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
# 		=> 토픽에 데이터가 들어왔는지, 들어오고 있는지 확인 가능. 커넥트가 bulk 모드면, 항상 전체 db 내용이 주기적으로 들어온다.
# 		=> 데이터 확인도 "컨슘"하는 행위이므로 컨슈머 쉘을 사용하여 확인하는 것이다.

# 결과:

connection.url=jdbc:sqlserver://${SET-DB-IP-HERE};databaseName=${SET-DB-NAME-HERE}
connection.user=${SET-DB-USER-HERE}
connection.password=${SET-DB-PASS-HERE}

# node values: [, bulk, timestamp, incrementing,timestamp+incrementing]
mode=bulk
incrementing.column.name=${SET-DB-KEY-COLUMN-HERE}

# 토픽 이름 지정
topic.prefix=test-jdbc-
# 브로커에서 별도 토픽 설정이 없으면, 데이터가 들어갈 떄 토픽이 자동생성된다.
# 이 때 여기서 지정한 프리픽스를 사용한다. 프리픽스 뒤에는 Table이름이 들어간다. e.g.)test-jdbc-{Table명}
# 토픽 안의 데이터는 json 형태로 저장되고, 'payload' key의 value에 본 내용이 들어간다. 이외 key는 메타데이터



# Define when identifiers should be quoted in DDL and DML statements.
# The default is 'always' to maintain backward compatibility with prior versions.
# Set this to 'never' to avoid quoting fully-qualified or simple table and column names.
#quote.sql.identifiers=always

