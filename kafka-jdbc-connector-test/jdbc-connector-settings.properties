#
# Copyright 2018 Confluent Inc.
#
# Licensed under the Confluent Community License (the "License"); you may not use
# this file except in compliance with the License.  You may obtain a copy of the
# License at
#
# http://www.confluent.io/confluent-community-license
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OF ANY KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations under the License.
#

# A simple example that copies all tables from a SQLite database. The first few settings are
# required for all connectors: a name, the connector class to run, and the maximum number of
# tasks to create:
name=test-source-sqlite-jdbc-autoincrement
connector.class=io.confluent.connect.jdbc.JdbcSourceConnector
tasks.max=1
# The remaining configs are specific to the JDBC source connector. In this example, we connect to a
# SQLite database stored in the file test.db, use and auto-incrementing column called 'id' to
# detect new rows as they are added, and output to topics prefixed with 'test-sqlite-jdbc-', e.g.
# a table called 'users' will be written to the topic 'test-sqlite-jdbc-users'.

##################################################
# 메모
##################################################
# - ms sql server 연결
# 	=> https://docs.confluent.io/kafka-connect-jdbc/current/source-connector/source_config_options.html
# 	=> jdbc 커넥터 다운로드시 포함된 예제 속성파일은 완전하지 않다. 위 링크 참고하여 필요한 세팅값을 더 넣어주자.

# 	=> 이슈1: wsl에서 커넥트 테스트 실행시킬때 windows에 있는 db 주소는 localhost가 아님. 네트워크 인터페이스 정확하게 넣어줘야 한다.
# 	=> 이슈2: 사용자가 로그인하지 못했습니다.
# 		=> 커넥터 속성파일에 connection.user, connection.password 등록 필요. 기본 속성파일 템플릿에 없음. 위 링크 참고
# 	=> 이슈3: column.name: id 등으로 데이터 읽을 기준을 잡아준다.
# 	=> 이슈4: mode: 모드에 따라 어떤식으로 db값을 가져올지 정한다. id 순차대로 새로운것, 시간순대로 새로운것, 기존것 모두(bulk) 등
# 		=> bulk 모드의 경우, db에 polling 할 때마다 전체 데이터를 긁어온다.
# 	=> 이슈5: 기본 에제의 connection.url에서 'instance=SQLSERVER;' 부분을 지우기 (실행되다 멈춤)
#   => ms sql server 설정시 이슈 일부: https://xzio.tistory.com/73  (글 맨아래쪽 참고)
#


# ㅇ- 카프카 브로커에 데이터가 들어갔는지 확인
# 	- 카프카 명령 모음 (https://pinggoopark.tistory.com/6)
# 	ㅇ- 토픽 리스트 확인 : $./bin/kafka-topics.sh --list --bootstrap-server localhost:9092
# 		=> 미리 토픽을 등록해두지 않으면 connect 설정값에 따라 자동 등록되므로, 새롭게 토픽이 추가된 것을 확인한다.
# 	ㅇ- 토픽의 데이터 확인 (컨슈머): $./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
# 		=> 토픽에 데이터가 들어왔는지, 들어오고 있는지 확인 가능. 커넥트가 bulk 모드면, 항상 전체 db 내용이 주기적으로 들어온다.
# 		=> 데이터 확인도 "컨슘"하는 행위이므로 컨슈머 쉘을 사용하여 확인하는 것이다.

connection.url=jdbc:sqlserver://${ip_address:port};databaseName=${db_name}
connection.user=${db_user}
connection.password=${db_password}

# node values: [, bulk, timestamp, incrementing,timestamp+incrementing]
mode=bulk
incrementing.column.name=CustomerId
topic.prefix=test-jdbc-

# Define when identifiers should be quoted in DDL and DML statements.
# The default is 'always' to maintain backward compatibility with prior versions.
# Set this to 'never' to avoid quoting fully-qualified or simple table and column names.
#quote.sql.identifiers=always

